---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = here::here(),
  out.width = "100%"
)

render_toc <- function(
  filename, 
  toc_header_name = "Table of Contents",
  base_level = NULL,
  toc_depth = 3
) {
  x <- readLines(filename, warn = FALSE)
  x <- paste(x, collapse = "\n")
  x <- paste0("\n", x, "\n")
  for (i in 5:4) {
    regex_code_fence <- paste0("\n[`]{", i, "}.+?[`]{", i, "}\n")
    x <- gsub(regex_code_fence, "", x)
  }
  x <- strsplit(x, "\n")[[1]]
  x <- x[grepl("^#+", x)]
  if (!is.null(toc_header_name)) 
    x <- x[!grepl(paste0("^#+ ", toc_header_name), x)]
  if (is.null(base_level))
    base_level <- min(sapply(gsub("(#+).+", "\\1", x), nchar))
  start_at_base_level <- FALSE
  x <- sapply(x, function(h) {
    level <- nchar(gsub("(#+).+", "\\1", h)) - base_level
    if (level > toc_depth - 1) return("")
    if (!start_at_base_level && level == 0) start_at_base_level <<- TRUE
    if (!start_at_base_level) return("")
    if (grepl("\\{#.+\\}(\\s+)?$", h)) {
      # has special header slug
      header_text <- gsub("#+ (.+)\\s+?\\{.+$", "\\1", h)
      header_slug <- gsub(".+\\{\\s?#([-_.a-zA-Z]+).+", "\\1", h)
    } else {
      header_text <- gsub("#+\\s+?", "", h)
      header_text <- gsub("\\s+?\\{.+\\}\\s*$", "", header_text) # strip { .tabset ... }
      header_text <- gsub("^[^[:alpha:]]*\\s*", "", header_text) # remove up to first alpha char
      header_slug <- paste(strsplit(header_text, " ")[[1]], collapse="-")
      header_slug <- tolower(header_slug)
    }
    paste0(strrep(" ", level * 4), "- [", header_text, "](#", header_slug, ")")
  })
  x <- x[x != ""]
  knitr::asis_output(paste(x, collapse = "\n"))
}
```

```{css, echo = FALSE}
.author,.title{
    display: none;
}
code span.co {
    color: #9897c7;
    font-weight: normal;
    font-style: italic;
}
code span.kw {
    color: #10aff2;
    font-weight: bold;
}
code span.st {
    color: #09e2c5;
}
```

<a href="https://mercadoedu.com.br">
    <img src="./inst/readme/images/slogan.png" align = "left" height = "59px"/>
</a>
<a href="https://tawk.to/fcs.est">
  <img src="./inst/readme/images/perfil.png" align = "right" height = "100px"/>
</a>

# Text Classification


<!-- badges: start -->
![Development Status](https://img.shields.io/badge/lifecycle-experimental-orange.svg)
<!-- badges: end -->

Repositório do modelo de classificação de texto dos nomes de curso da mercadoedu, utilizando o algoritmo [TF-IDF](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) da biblioteca [scikit-learn](https://scikit-learn.org/).

# Sumário
```{r toc, echo=FALSE}
render_toc(filename = "./README.Rmd",
           toc_header_name = "Sumário",
           base_level = 2,
           toc_depth = 4)
```  

## Clonando repositório

Para clonar o repositório e visualizar as rotinas utilize os comandos abaixo conforme o gif:

```{bash eval=FALSE, echo=TRUE}
sudo git clone https://github.com/fcsestme/mercadoedu.tc.git]
```

```{bash eval=FALSE, echo=TRUE}
sudo cd mercadoedu.tc

ls
```

## Rotina principal

Temos a rotina principal chamada `./main.sh` que roda as demais rotinas contidas nas pastas `./python` e `./scripts`.

Após entrarmos no diretório clonado, chamamos a rotina principal com o comando abaixo:

```{bash eval=FALSE, echo=TRUE}
./main.sh
```

Desta forma irá aparecer uma série de alternativas que irão rodar as demais rotinas necessárias e para escolher uma opção digite o respectivo número da escolha e pressione `Enter`.

As três primeiras opções(`1`, `2` e `3`) só precisam ser rodadas na máquina pela primeira vez.

As opções de `Treinar modelos` e `Avaliar modelos`(`4` e `5`) seriam duas rotinas, uma diária e uma semanal respectivamente.

As últimas opções(`9` e `0`) são para sair do terminal e do menu respectivamente.

### Depedências

A primeira opção(`1`) irá instalar as dependências necessárias para rodar todas as demais rotinas. 

Abaixo você pode conferir o passo a passo dessa rotina de instalação de dependências.

#### Atualizando Ubuntu
#### Atualizando Python
#### Instalando bibliotecas em Python
#### Baixando as stopwords em Português

### Variáveis de ambiente

A segunda opção(`2`) irá abrir um arquivo `./.env` para você definir as variáveis de ambiente.

### Autenticação do AWS

A terceira opção(`3`) irá rodar a função de configuração do AWS, para realizar a autenticação.

Desta forma após selecioná-la, será solicitado o AWS Access Key ID, AWS Secret Access Key, o nome da região padrão(por padrão seria `us-east-2`) e o formato de saída padrão(por padrão seria `json`).

### Treinamento dos modelos
#### Pré-processamento
#### Vetorização
#### Classificador
#### Deploy

### Avaliação dos modelos
#### Pré-processamento
#### Extração de features
#### Seleção de features
#### Seleção de modelos
#### Avaliação do modelo
#### Explicabilidade do modelo
#### Complexidade vs interpretabilidade

## Referências

Todo código contido neste projeto foi desenvolvido a partir das referências abaixo:

- https://reslan-tinawi.github.io/2020/05/26/text-classification-using-sklearn-and-nltk.html
